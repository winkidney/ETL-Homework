# ETL-Homework

![Tests Status](https://github.com/winkidney/ETL-Homework/actions/workflows/py-tests.yml/badge.svg)

This ETL workflow is based on SaaS and open-source software named "prefect".
It provides simple yet powerful API and DAG based workflow manager: scheduler and task executor.
We could take much advantage from following features:
+ Both Self-Hosted or SaaS is available
+ Simple alert support based on Email
+ DAG based workflow support, with scheduler and executor
+ Work-Pools support, make it easy to do up scaling
+ Simple error handling

We could easily start our data ETL flow, and scale to complex workflow.
Classic tools like `Kafka`, `Hive`, `Elasticsearch` and `Spark` is also available in prefect workflow.

# Core Features
## Features Done This Version
+ [x] E: Extract data by API crawler 
  + [x] Cold start with history data
  + [x] Three types of data crawler: hash-rate, price, network-difficulty 
+ [x] T: Transform
  + [x] Transform data and make new data table in specified 
+ [x] L: Load data
  + [x] Database storage for Load feature (for example DuckDB)
  + [x] CSV file output for other OLAP
+ [x] Monitoring
  + [x] Email alert for task error
+ [x] Extra: auto report with email

## TODO
+ [ ] Auto data correction for possible bad data
+ [ ] Deployment features on cloud or other methods
+ [ ] Full history API crawler for full-history-detail
+ [ ] Visualize of DAG flow
+ [ ] Classic BigData tools support
+ [ ] Production-ready docker configuration

# Core Design Principles
For new products, the ultimate goal is to quickly and lightly launch the business itself.
My design is always follow the rules below:
+ `Design More, Write Less`: It helps us save time and money. 
+ `Write code that is easy to delete, not easy to extend`: Embrace change, adapt to change, and iterate quickly.
+ `Write Tests, Gain Courage`: Only better tests can make you iterate quickly. 

# Simple Usage with Docker
You can use docker-compose to simplify env configuration.
**Note**: This docker-compose file is not ready for production, it is just for simplify the process we bootstrap the project. 

You may first have docker installed, and then:
```shell
# copy file or edit the configurations
cp docker-compose.example.yml docker-compose.yml
# bootstrap, in this step you should first register an account
# on prefect.io and create an API key in user control panel.
# Please open `Prefect Dashboard-> default -> API keys -> create one`
# and then paste the API-Key below
docker-compose run bootstrap
# start local task
docker-compose up consumer
# in another terminal
docker-compose run export-report
# You will get:
# Creating etl-homework_export-report_run ... done
# poetry run python -m etl_homework.handy.export_to_local_dir
# exported to:  coin_metrics_2024-10-29T16-29-15.253090.csv
```

**Note2**: You could run any command in `Development` part by attach to docker container.
**Note3**: If binance API report error, please config `ETL_HTTP_PROXY` and `ETL_HTTPS_PROXY` in `.env` file.

# Development

**Note**: be sure you have python>=3.9 installed

Install requires:
```shell
# poetry is required for development and python>=3.9 is required
pip install poetry
# install env
poetry install

# copy git hook
cp ./scripts/pre-commit.example ./git/hooks/pre-commit

# configure local settings
cp .env.example .env
# then edit it with your favorite editor ( for me it is nano )
# then change the alert email address
# nano .env 
```

Test:
```shell
make test
```

Run on cloud:
```shell
# switch to cloud and login with API-KEY or browser
# note: you should first register an account at https://www.prefect.io/
# visit "signup/login" on landing page of prefect.io
# then follow the link created by following command to finish the setup
make switch2cloud

# create local databases
make sync-db 

# then run task-entry
make local-task
```

Run on local: 
Currently not fully supported, `Task Failure Alert` feature will not work.  

# Deploy
Now we only use local process to serve `depolyment` instance since we have limited calculation and data fetching.

just run:

```shell
make local-task
```

It will make the deployment done.

# End-User Manual
## Error Report

Please setup an automation on platform, just like this:
![Create Automation for Task-Failure-Alert](./res/imgs/alert-conf.jpg)


## Get full CSV report in Email
At first, create credentials to an EmailServerCredentials block
```shell
make register-email-block
# will output a link to register a block
```
Follow the link generated by command above, then search "Email Server Credentials" to create a gmail provider (using Google App Password is recommended).
**Note**: the block name now is hard coded as `etl-gmail-sender`  

![Create Email Provider](./res/imgs/gmail-conf.jpg)

If you want to generate a full-report which will be sending to your Email immediately, you may just run:

```shell
# be sure that your network has access to google services
make send-full-report
```

You should have local-task executed more than 5 minutes (except that we will have no data in DB).


## Analysis

Run following command, export a local CSV file which could be imported by almost all OLAP.
```
make export-to-local
```
You could import the full csv to tools like `DuckDB` or any other OLAP (Excel is very ok, HaHa!).

The `generated_data.sqlite` is also a good data source, with clean data.
